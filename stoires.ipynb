{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marked_words import marked_words\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def pprint(dic):\n",
    "    full_list = []\n",
    "    for word in sorted(dic,key=lambda x: x[1],reverse=True):\n",
    "#         print(\"%s, %.2f\" % (word[0],word[1]))\n",
    "        full_list.append(word[0])\n",
    "    return full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "owned-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('data/dv3/dv3_story_generations.csv')\n",
    "df2= pd.read_csv('data/dv3_generations_nb_stories.csv')\n",
    "df2=df2.drop(0)\n",
    "# df2=df2.loc[(df2['prom/pt_num']==4)|(df2['prompt_num']==5)]\n",
    "df2['race']=df2['race'].replace('a Latino', 'a Latine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "talented-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df1,df2])\n",
    "\n",
    "\n",
    "df = df.loc[df['text'].notna()]\n",
    "df.to_csv('data/dv3_story_generations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "raising-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marked_words import marked_words\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def pprint(dic):\n",
    "    full_list = []\n",
    "    for word in sorted(dic,key=lambda x: x[1],reverse=True):\n",
    "#         print(\"%s, %.2f\" % (word[0],word[1]))\n",
    "        full_list.append(word[0])\n",
    "    return full_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "canadian-executive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top words for a White N \n",
      "-------\n",
      "\n",
      " Top words for a White W \n",
      "-------\n",
      "\n",
      " Top words for a Black N \n",
      "-------\n",
      "\n",
      " Top words for a Black W \n",
      "-------\n",
      "\n",
      " Top words for an Asian N \n",
      "-------\n",
      "\n",
      " Top words for an Asian W \n",
      "-------\n",
      "\n",
      " Top words for a Middle-Eastern N \n",
      "-------\n",
      "\n",
      " Top words for a Middle-Eastern W \n",
      "-------\n",
      "\n",
      " Top words for a Latine N \n",
      "-------\n",
      "\n",
      " Top words for a Latine W \n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "dv3_mw = {}\n",
    "for race in df['race'].unique():\n",
    "#     print('\\n Top words for %s \\n-------' % race)\n",
    "    outs = pprint(marked_words(df, [race], ['race'],['a White']))\n",
    "    dv3_mw[race] = outs\n",
    "temps = []\n",
    "for race in df['race'].unique():\n",
    "#     print('\\n Top words for %s \\n-------' % race)\n",
    "    temp = pprint(marked_words(df, ['a White'], ['race'],[race]))\n",
    "    temps.extend(temp)\n",
    "seen = Counter(temps).most_common()\n",
    "dv3_mw['a White']=[w for w, c in seen if c == 4]\n",
    "\n",
    "\n",
    "for race in df['gender'].unique():\n",
    "#     print('\\n Top words for %s \\n-------' % race)\n",
    "    outs = pprint(marked_words(df, [race], ['gender'],['M']))\n",
    "    dv3_mw[race] = outs\n",
    "temps = []\n",
    "for race in df['gender'].unique():\n",
    "#     print('\\n Top words for %s \\n-------' % race)\n",
    "    temp = pprint(marked_words(df, ['M'], ['gender'],[race]))\n",
    "    temps.extend(temp)\n",
    "\n",
    "seen = Counter(temps).most_common()\n",
    "dv3_mw['M']=[w for w, c in seen if c == 2]\n",
    "    \n",
    "    \n",
    "# Top words for intersectional groups\n",
    "for race in df['race'].unique():\n",
    "    for gen in ['N','W']:\n",
    "        print('\\n Top words for %s %s \\n-------' % (race,gen))\n",
    "        dv3_mw[race+gen] = pprint(marked_words(df, [race, gen], ['race', 'gender'],['a White','M']))\n",
    "        \n",
    "#     dv3_mw[race] = outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abroad-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RACE\n",
      "Mean accuracy across race groups: 0.83 ± 0.06\n",
      "GENDER\n",
      "Mean accuracy across gender groups: 0.87 ± 0.08\n",
      "RACEGENDER\n",
      "Mean accuracy across racegender groups: 0.94 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import sklearn.feature_selection \n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(binary = True, decode_error = u'ignore')\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "def anonymize(bio, remove_names=True, remove_gender_markers=True, remove_title=True, replacement=\"\"):\n",
    "    bio = re.sub(r\"\\b(?:[Hh]e|[Ss]he|[Hh]er|[Hh]is|[Hh]im|[Hh]ers|[Hh]imself|[Hh]erself|hes|shes|[Mm][Rr]|[Mm][Rr][sS]|[Mm][Ss]|man|male|bro|bros)\\b\", replacement, bio)\n",
    "    bio = re.sub(r\"african|middleeastern|middleeast|spanishspeaking|mexico|spanish|african-american|black|hispanic|latinx|latine|latina|latino|latin|asian|asian-american|desi|european|europe|asia|middle eastern|arab|white|caucasian|arabic|aapi|bipoc|filipin*|mexic*|india|salvador|cuban|chinese|japanese|korean|china\", replacement, bio)\n",
    "    bio = re.sub(r\"female|genderconforming|cisgender|cis|cisgender|descriptors|AFAB|AMAB|androgynous|butch|effeminate|feminine|femme|manly|masculine|womanly||female|woman|women|lady|ladies|girl|girls|mother|mothers|mom|moms|daughter|daughters|wife|wives|grandmother|grandmothers|grandma|grandmas|sister|sisters|male|bros|guy|guys|boy|boys|father|fathers|dad|dads|son|sons|husband|husbands|grandfather|grandfathers|grandpa|grandpas|brother|brothers\", replacement, bio)\n",
    "    return bio\n",
    "\n",
    "alldata = df.copy()\n",
    "alldata['racegender'] = alldata['race']+alldata['gender']\n",
    "data = alldata['text'].str.lower().replace('[^\\w\\s]','',regex=True)\n",
    "top_words = dict()\n",
    "dv3_svm = {}\n",
    "for st in ['race','gender','racegender']:\n",
    "    print(st.upper())\n",
    "    for d in data:\n",
    "        try:\n",
    "            anonymize(d)\n",
    "        except TypeError:\n",
    "            print(d)\n",
    "    concept_data = [anonymize(d) for d in data]\n",
    "\n",
    "    labels = alldata[st]\n",
    "\n",
    "    bios_data_train, bios_data_test,Y_train,Y_test = train_test_split(concept_data, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word',min_df=0.001,binary=False)\n",
    "    X_train = vectorizer.fit_transform(bios_data_train)\n",
    "    X_test = vectorizer.transform(bios_data_test)\n",
    "    accs = []\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for r in alldata[st].unique():\n",
    "        svm = SVC(kernel='linear')\n",
    "        Y_train_bin = Y_train==r\n",
    "        svm.fit(X_train, Y_train_bin)\n",
    "        acc=sklearn.metrics.accuracy_score(Y_test==r,svm.predict(X_test))\n",
    "#         print(\"%s Accuracy: %.2f\"%(r,acc))\n",
    "        accs.append(acc)\n",
    "        coef = svm.coef_.toarray()[0]\n",
    "        _, names = zip(*sorted(zip(coef,feature_names)))\n",
    "#         print(\"Top 10 words: %s\" % str(names[-10:][::-1]))\n",
    "        dv3_svm[r] = names[-10:][::-1]\n",
    "    print(\"Mean accuracy across %s groups: %.2f ± %.2f\"%(st,np.mean(accs),np.std(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bright-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=['a White',\n",
    " 'a Black',\n",
    " 'an Asian',\n",
    " 'a Middle-Eastern',\n",
    " 'a Latine',\n",
    " 'M',\n",
    " 'W','N',\n",
    " 'a WhiteW',\n",
    " 'a BlackW',\n",
    " 'an AsianW',\n",
    " 'a Middle-EasternW',\n",
    " 'a LatineW','a WhiteN', 'a BlackN', 'an AsianN', 'a Middle-EasternN', 'a LatineN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "suffering-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps={\n",
    "    \n",
    "    'a White':'White', 'a Black':'Black', 'an Asian':'Asian', 'a Middle-Eastern':'ME', 'a Latine':'Latine', 'M':'\\hline man', 'W':'woman', 'N':'nonbinary', 'a BlackW':'\\\\hline Black W', 'an AsianW':'Asian W', 'a Middle-EasternW':'ME W', 'a LatineW':'Latine W',\n",
    "'a BlackN':'Black NB', 'an AsianN':'Asian NB', 'a Middle-EasternN':'ME NB', 'a LatineN':'Latine NB'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "terminal-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = {}\n",
    "flags=[]\n",
    "for k in dv3_mw:\n",
    "#     if ('N' not in k and 'WhiteW' not in k) or len(k)==1:\n",
    "        boths = []\n",
    "#         only = []\n",
    "        for v in dv3_mw[k]:\n",
    "            if v in dv3_svm[k]:\n",
    "                tempword = '\\\\textit{%s}'%v\n",
    "            else:\n",
    "                tempword = v\n",
    "            boths.append(tempword)\n",
    "#             if v in dv3_mw[k]:\n",
    "#                 boths.append(tempword)\n",
    "#             else:\n",
    "#                 only.append(tempword)\n",
    "#         if len(boths)>= 20:\n",
    "#             final_table[k]='\\hl{%s}'%(', '.join(boths[:20]))\n",
    "# #         print(k)\n",
    "# #         print(len(boths))\n",
    "#         if len(boths)<20:\n",
    "#             max_length = 20\n",
    "#             only_max = max_length - len(boths)\n",
    "#             if len(boths)<only_max:\n",
    "#                 num_needed = only_max-len(boths)\n",
    "#                 flags.append((k,num_needed))\n",
    "                \n",
    "#             only = only[:min(len(only),only_max)]\n",
    "        final_table[k]=', '.join(boths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "coordinate-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a White\n",
      "a Black\n",
      "an Asian\n",
      "a Middle-Eastern\n",
      "a Latine\n",
      "M\n",
      "W\n",
      "N\n",
      "a WhiteN\n",
      "a WhiteW\n",
      "a BlackN\n",
      "a BlackW\n",
      "an AsianN\n",
      "an AsianW\n",
      "a Middle-EasternN\n",
      "a Middle-EasternW\n",
      "a LatineN\n",
      "a LatineW\n"
     ]
    }
   ],
   "source": [
    "extras={}\n",
    "for k in dv3_mw:\n",
    "    print(k)\n",
    "    svm_list= dv3_svm[k]\n",
    "#     num_got = 0\n",
    "    extra_words=[]\n",
    "    for word in svm_list:\n",
    "#         if num_got < num_n:\n",
    "        if word not in final_table[k]:\n",
    "#                 num_got +=1\n",
    "#                 print(word)\n",
    "            extra_words.append(word)\n",
    "#     print(len(extra_words))\n",
    "#     print(num_n)\n",
    "    if len(extra_words)>0:\n",
    "        extras[k] = '\\\\textcolor{gray}{(%s)}'%', '.join(extra_words)\n",
    "    else:\n",
    "        extras[k]=''\n",
    "# print(extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "supreme-kennedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White &white, john, megan, \\textcolor{gray}{(sam, out, jack, group, town, mac, understood, over, lila, emi)}\\\\\n",
      "Black &black, tyler, \\textit{nathaniel}, ryder, \\textcolor{gray}{(others, jane, nina, jeremiah, kiara, where, went, only, into)}\\\\\n",
      "Asian &asian, i, \\textit{ling}, \\textit{mei}, \\textit{li}, \\textit{kai}, china, my, \\textit{takashi}, beijing, martial, arts, \\textit{hua}, shii, wei, shanghai, \\textit{tomo}, \\textcolor{gray}{(yujin, chen, city)}\\\\\n",
      "ME &\\textit{middle}, middleeastern, \\textit{ali}, \\textit{east}, \\textit{hassan}, eastern, \\textit{ahmed}, \\textit{village}, \\textit{farrah}, \\textit{farid}, culture, saeed, fatima, desert, \\textcolor{gray}{(began, country)}\\\\\n",
      "Latine &latino, \\textit{maria}, latina, \\textit{juan}, mexico, hard, \\textit{marisol}, \\textit{veronica}, carlos, states, \\textit{rafael}, worked, latin, mexican, \\textit{determined}, her, jose, antonio, united, business, \\textcolor{gray}{(identity, sole, josé, javier)}\\\\\n",
      "\\hline man &he, his, him, man, himself, \\textit{john}, ali, \\textit{juan}, \\textit{takashi}, hed, james, jack, \\textit{carlos}, farid, \\textit{rafael}, martial, marco, jose, \\textcolor{gray}{(ricardo, martin, work, american, been)}\\\\\n",
      "woman &she, her, woman, herself, women, \\textit{mei}, latina, \\textit{maria}, \\textit{li}, career, \\textit{nina}, marisol, independent, \\textit{shed}, \\textit{dreams}, \\textit{fatima}, elizabeth, \\textcolor{gray}{(determined, how, firm)}\\\\\n",
      "nonbinary &\\textit{they}, \\textit{their}, \\textit{nonbinary}, \\textit{identity}, \\textit{gender}, them, were, themselves, \\textit{felt}, person, \\textit{fit}, her, she, like, express, i, quite, acceptance, accepted, who, true, or, didnt, embraced, traditional, binary, accepting, supportive, understand, either, roles, my, self, community, pronouns, judgement, neither, understood, female, male, friends, understanding, labels, people, identified, be, it, queer, accept, expectations, belonging, safe, expression, shii, nathaniel, ryder, tomo, truth, \\textcolor{gray}{(alice, family)}\\\\\n",
      "\\hline Black W &her, she, black, \\textit{sheila}, \\textcolor{gray}{(only, calista, on, career, patrice, lashauna, slowly, stella, kara)}\\\\\n",
      "Asian W &her, she, \\textit{mei}, \\textit{li}, \\textit{ling}, asian, \\textcolor{gray}{(cultural, boss, jinyan, liang, business, ahn, often)}\\\\\n",
      "ME W &her, \\textit{fatima}, \\textcolor{gray}{(village, amina, saba, society, determined, would, aneesa, noora, saraya)}\\\\\n",
      "Latine W &her, she, \\textit{maria}, latina, \\textit{marisol}, linda, \\textcolor{gray}{(lupita, determined, lizette, mariye, consuela, miami, library, after)}\\\\\n",
      "Black NB &they, their, \\textit{nathaniel}, ryder, mica, \\textcolor{gray}{(jane, athena, kiara, darwin, found, lidia, loved, go, other)}\\\\\n",
      "Asian NB &they, their, i, asian, my, \\textit{kai}, \\textit{shii}, tomo, yui, \\textit{ade}, kim, \\textcolor{gray}{(being, niko, for, jai, kiku, community, different)}\\\\\n",
      "ME NB &their, they, aziz, \\textit{mabrouk}, habib, \\textcolor{gray}{(began, hassan, ayah, gender, rafaela, farrah, mazen, nour, strict)}\\\\\n",
      "Latine NB &their, they, \\textit{identity}, \\textit{antonio}, veronica, latinx, \\textit{mauricio}, \\textcolor{gray}{(nonbinary, lino, isabel, sabrina, natalia, sole, could)}\\\\\n"
     ]
    }
   ],
   "source": [
    "# maps = \n",
    "for k in rows:\n",
    "    v = final_table[k]\n",
    "# #     if k\n",
    "#     if k in extras:\n",
    "    try:\n",
    "        print(maps[k]+' &'+v+', '+extras[k]+'\\\\\\\\')\n",
    "    except KeyError:\n",
    "        continue\n",
    "# #     else:\n",
    "#         try:\n",
    "#     #             print(maps[k])\n",
    "#             print(maps[k]+' &'+v+'\\\\\\\\')\n",
    "#         except KeyError:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-organization",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
